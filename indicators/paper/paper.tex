\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
% Use the lineno option to display guide line numbers if required.

\templatetype{pnasresearcharticle} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} %= Template for a one-column mathematics article
% {pnasinvited} %= Template for a PNAS invited submission

\input{../../common/prelim.tex}

\title{An Open Repository of Real-Time COVID-19 Indicators}

% Use letters for affiliations, numbers to show equal authorship (if applicable)
% and to indicate the corresponding author

% Writing group
\author[a,1]{Alex Reinhart}
\author[b,2]{Logan Brooks}
\author[a,b,2]{Maria Jahja}
\author[b,2]{Aaron Rumack}
\author[c,2]{Jingjing Tang}

% Middle authors
\author[d]{Sumit Agrawal}
\author[e]{Wael Al Saeed}
\author[f]{Taylor Arnold}
\author[g]{Amartya Basu}
\author[h]{Jacob Bien}
\author[i]{{\'A}ngel A. Cabrera}
\author[b]{Andrew Chin}
\author[b]{Eu Jing Chua}
\author[b]{Brian Clark}
\author[d]{Sarah Colquhoun}
\author[b]{Nat DeFries}
\author[d]{David C. Farrow}
\author[i]{Jodi Forlizzi}
\author[d]{Jed Grabman}
\author[b]{Samuel Gratzl}
\author[a]{Alden Green}
\author[b]{George Haff}
\author[i]{Robin Han}
\author[d]{Kate Harwood}
\author[a,b]{Addison J. Hu}
\author[d]{Raphael Hyde}
\author[h]{Sangwon Hyun}
\author[e]{Ananya Joshi}
\author[j]{Jimi Kim}
\author[i]{Andrew Kuznetsov}
\author[b]{Wichada La Motte-Kerr}
\author[i]{Yeon Jin Lee}
\author[k]{Kenneth Lee}
\author[b]{Zachary C. Lipton}
\author[i]{Michael X. Liu}
\author[l]{Lester Mackey}
\author[b]{Kathryn Mazaitis}
\author[m]{Daniel J. McDonald}
\author[d]{Phillip McGuinness}
\author[n,o]{Balasubramanian Narasimhan}
\author[d]{Michael P. O'Brien}
\author[a,b]{Natalia L. Oliveira}
\author[a,b]{Pratik Patil}
\author[i]{Adam Perer}
\author[b]{Collin A. Politsch}
\author[n]{Samyak Rajanala}
\author[e]{Dawn Rucker}
\author[d]{Chris Scott}
\author[p]{Nigam H. Shah} 
\author[q]{Vishnu Shankar} 
\author[k]{James Sharpnack}
\author[b]{Dmitry Shemetov}
\author[r]{Noah Simon} 
\author[d]{Benjamin Y. Smith}
\author[b]{Vishakha Srivastava}
\author[m]{Shuyi Tan}
\author[n,o]{Robert Tibshirani}
\author[n]{Elena Tuzhilina}
\author[b]{Ana Karina Van Nortwick}
\author[a]{Val{\'e}rie Ventura}
\author[a,b]{Larry Wasserman}
\author[d]{Benjamin Weaver}
\author[s]{Jeremy C. Weiss} 
\author[d]{Spencer Whitman}
\author[i]{Kristin Williams}

% Senior authors
\author[b]{Roni Rosenfeld}
\author[a,b]{Ryan J. Tibshirani}

\affil[a]{Department of Statistics \& Data Science, Carnegie Mellon University}
\affil[b]{Machine Learning Department, Carnegie Mellon University}
\affil[c]{Computational Biology Department, Carnegie Mellon University}
\affil[d]{Google.org}
\affil[e]{Computer Science Department, Carnegie Mellon University}
\affil[f]{Linguistics Program, University of Richmond}
\affil[g]{Information Networking Institute, Carnegie Mellon University}
\affil[h]{Department of Data Sciences and Operations, University of Southern California}
\affil[i]{Human-Computer Interaction Institute, Carnegie Mellon University}
\affil[j]{School of Natural Sciences and Mathematics, University of Texas at Dallas}
\affil[k]{Department of Statistics, University of California, Davis}
\affil[l]{Microsoft Research New England}
\affil[m]{Department of Statistics, University of British Columbia}
\affil[n]{Department of Statistics, Stanford University}
\affil[o]{Department of Biomedical Data Science, Stanford University}
\affil[p]{Department of Medicine, Stanford University}
\affil[q]{Program in Immunology, Stanford University School of Medicine}
\affil[r]{Department of Biostatistics, University of Washington}
\affil[s]{Heinz College of Information Systems and Public Policy, Carnegie
  Mellon University}

% Please give the surname of the lead author for the running footer
\leadauthor{Reinhart}

% Please add a significance statement to explain the relevance of your work
\significancestatement{To study the COVID-19 pandemic, its effects on society,
  and measures for reducing its spread, researchers need detailed data on the
  course of the pandemic. Standard public health data streams suffer
  inconsistent reporting and frequent, unexpected revisions. They also miss
  other aspects of a population's behavior that are worthy of consideration. We
  present an open database of COVID signals in the United States, measured at
  the county level and updated daily. This includes traditionally reported COVID
  cases and deaths, and many others: measures of mobility, social distancing,
  internet search trends, self-reported symptoms, and patterns of COVID-related
  activity in de-identified medical insurance claims. The database provides all
  signals in a common, easy-to-use format, empowering both public health
  research and operational decision-making.
}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Author contributions: A.R., L.B., M.J., A.R., J.T., A.C,
  E.J.C., N.D., D.C.F, A.J.H., S.H., M.J., J.K., W.L.M.K, Z.C.L., L.M., N.L.O.,
  J.C.W, R.R., and R.J.T. obtained and managed data;
  A.R., J.B., J.F., Z.C.L., L.M., D.J.M., A.P., S.R., D.R., C.S. N.S., R.T.,
  V.V., L.W., B.W., J.C.W, S.W., R.R., and R.J.T. designed software and
  research;
  A.R., L.B., M.J., A.R., J.T., W.A.S., T.A., A.B., J.B., L.B., A.A.C., A.C.,
  E.J.C., B.C., S.C., N.D., D.C.F., J.G., S.G., A.G., G.H., R.H., K.H., A.J.H.,
  S.H., M.J., A.J., J.K., A.K., M.X.L., L.M., K.M., D.J.M., B.N., M.P.O.,
  N.L.O., P.P., C.A.P., S.R., C.S., V.S, J.S, D.S., N.S., B.Y.S., V.S., S.T.,
  J.T., R.T., J.C.W, and R.J.T. developed software;
  S.A., W.A.S., A.C., B.C., D.C.F., S.G., G.H., K.M., D.J.M., B.N., C.S., J.S.,
  D.S., B.Y.S., and A.K.V.N. managed technical infrastructure and systems;
  A.R., L.B., M.J., A.R., J.T, J.B., A.C., E.J.C., N.D., D.C.F., A.G., K.H.,
  A.J.H., R.H., S.H., A.J., J.K., K.L., Y.J.L, Z.C.L., L.M., D.J.M., P.M., B.N.,
  N.L.O., P.P., C.A.P., S.R., N.S., V.S., J.S., D.S., N.S., S.T., J.T., R.T.,
  V.V., L.W., J.C.W., R.R., and R.J.T. performed research;
  R.R. and R.J.T. acquired funding;
  and A.R., L.B., M.J., A.R., J.T., and R.J.T. wrote the paper.
}
\authordeclaration{No competing interests to declare.}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be
  addressed. E-mail: areinhar@stat.cmu.edu}
\equalauthors{\textsuperscript{2}L.B., M.J., A.R., and J.T. contributed equally
  to this work.}

% At least three keywords are required at submission. Please provide three to five keywords, separated by the pipe symbol.
\keywords{COVID-19 $|$ open data $|$ digital surveillance $|$ medical insurance
  claims $|$ internet surveys}

\begin{abstract}
  The COVID-19 pandemic presented enormous data challenges in the United States.
  Policy makers, epidemiological modelers, and health researchers all require
  up-to-date data on the pandemic and relevant public behavior, ideally at fine
  spatial and temporal resolution. The COVIDcast API is our attempt to fill this
  need: operational since April 2020, it provides open access to both
  traditional public health surveillance signals (cases, deaths, and
  hospitalizations) and many auxiliary indicators of COVID-19 activity, such as
  signals extracted from de-identified medical claims data, massive online
  surveys, cell phone mobility data, and internet search trends. These
  are available at a fine geographic resolution (mostly at the county level) and
  are updated daily. The COVIDcast API also tracks all revisions to historical
  data, allowing modelers to account for the frequent revisions and backfill
  that are common for many public health data sources. All of the data is
  available in a common format through the API and accompanying R and Python
  software packages. This paper describes the data sources and signals, and
  provides examples demonstrating that the auxiliary signals in the COVIDcast
  API present information relevant to tracking COVID activity, augmenting
  traditional public health reporting and empowering research and
  decision-making.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\begin{document}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\dropcap{P}ublic health decision makers, healthcare providers, epidemiological
researchers, employers, institutions, and the general public benefit from
promptly and readily accessible data regarding COVID-19 activity levels,
countermeasures, and pandemic impact.  Real-time indicators of COVID-19
activity levels, such as statistics on cases, deaths, test positivity, and
hospitalizations, enable reports and interactive dashboard applications for
situational awareness \cite{Dong:2020, NYTimes, USAFacts}, and are essential for
most analyses of the pandemic.  These data are available for locations across
the United States from a number of official sources and independent aggregators
in varied and inconsistent formats.  Different data types and sources vary in
timeliness, based on when measured events occur in the progression of the
disease, testing capabilities, the reporting pipeline, and their publication
schedules.

Additional, auxiliary data sources can improve on the timeliness, scope, and
utility of the ``topline'' indicators (cases, test positivity, hospitalizations,
deaths) coming from the public health reporting system. For example, in the
context of other infectious diseases: syndromic surveillance in ambulatory
clinics and emergency rooms improves the accuracy of outbreak detection for
emerging pathogens such as H1N1 \cite{Kass-Hout:2012}; and digital surveillance
(based on, e.g., search and social media trends) enables more accurate
``nowcasts'' and forecasts of traditional disease surveillance streams such as
the CDC's ILINet \cite{Santillana:2015, Farrow:2016}, as do publication formats
providing access to historical versions of a given data set \cite{Brooks:2018,
  Brooks:2020}. Several other examples exist that span a wide variety of data
platforms and diseases \cite{Browstein:2009, Kass-Hout:2011, Salathe:2012,
  Kass-Hout:2013}. During the COVID-19 pandemic, digital data streams have
permitted faster prediction of case increases \cite{Ahmad:2020, Kogan:2021},
while enabling analyses of the impact of public health policies on public
behavior, the economy, and disease spread \cite{Bonaccorsi:2020, Nouvellet:2021,
  Adjodah:2021, Jewell:2021}.

The Delphi Group works with partner organizations and public data sets to build
a massive database of indicators tracking COVID-19 activity and other relevant
phenomena in the United States, which has been publicly available and
continuously updated since April 2020. Alongside public data on reported cases
and deaths, this database includes several unique data streams, including
indicators extracted from de-identified medical claims data, antigen test
results from a major testing manufacturer, large-scale public surveys that
measure symptoms and public behavior, and indicators based on particular Google
search queries. (We use the terms ``indicator'' and ``signal'' interchangeably.)
We make aggregate signals publicly available, generally at the county level, via
the COVIDcast API \cite{CovidcastAPI}. We store and provide access to all
previous (historical) versions of the signals, a key feature that exposes the
effects of data revisions.  Lastly, we provide R \cite{CovidcastR} and Python
\cite{CovidcastPy} packages to facilitate interaction with the API, and an
online dashboard to visualize the data \cite{CovidcastViz}.

In a companion paper, we analyze the utility provided by a core set of the
indicators in COVID-19 forecasting and hotspot prediction models.  In another
companion paper, we elaborate on our research group's (Delphi's) large-scale
public surveys, run in partnership with Facebook and available in aggregate form
in the COVIDcast API.

\section{Methods}

\subsection{Data Collection}

We receive data daily from healthcare partners, technology companies, and from
surveys conducted daily by Delphi in partnership with Facebook. These data
sources provide information not available from standard public health reporting
or other common sources, such as:

\paragraph{Health Insurance Claims} Based on de-identified medical insurance
claims from Change Healthcare and other health system partners, we release
indicators on the estimated percentage of covered outpatient visits and
hospitalizations that involved COVID diagnoses or symptoms.

\paragraph{Internet-Based Surveys} Conducted in partnership with Facebook,
Delphi's COVID Trends and Impact Survey receives an average of 50,000 responses
daily, and has received over 20 million responses since April 2020
\cite{DelphiSurvey, Kreuter:2020}. From the surveys, we construct indicators on
symptoms, social distancing, vaccination, and other attitudes and behaviors
related to COVID.

\paragraph{COVID Antigen Tests} Based on data from Quidel, a manufacturer of
COVID antigen tests in the United States, we calculate and release
(Quidel-specific) test volumes and positivity rates.

\paragraph{Search Trends} Based on Google's COVID-19 Search Trends data set
\cite{GoogleSymptoms}, we provide indicators reflecting COVID-related search
activity.

\paragraph{Mobility Data} SafeGraph, a company that collects geospatial data
from smartphone apps, calculates COVID-related mobility signals
\cite{SafeGraphSocial, SafeGraphPatterns} and makes them available to
researchers under a data use agreement; we aggregate (some of) these signals to
the county level and make them publicly available.

\medskip
We also scrape data accessible from other public sources, such as cases and
deaths data aggregated from public reporting by JHU CSSE \cite{Dong:2020} and by
USAFacts \cite{USAFacts}, so that we can track revisions and updates to this data
(see Section~\ref{subsec:revision_tracking}).

Altogether, we collect 110 signals from 12 distinct sources, aggregate and
process them, and provide them in a common format for public access. This
unifies both unique (not available anywhere else) and standard COVID data
streams into a single common format, enabling efficient comparison and modeling.
A summary of the data sources and signals in the API is in
Table~\ref{tab:sources_signals}, and detailed documentation is available at
\url{https://cmu-delphi.github.io/delphi-epidata/api/covidcast_signals.html}.

\begin{table*}[t]
\centering
\caption{Data sources available in Delphi's COVIDcast API \cite{CovidcastAPI},
  as of date of publication. The first group of data sources are produced by
  Delphi from data not otherwise available publicly (or only available in
  limited form); the second group is mirrored from public sources.}
\begin{tabular}{>{\raggedright}p{1.2in} p{4.0in} l >{\raggedright\arraybackslash}p{0.5in}}
  \toprule
  \textbf{Data source} & \textbf{Signals available} & \textbf{First date} &
\textbf{Resolution} \\\midrule
  % Exclusive data
  Change Healthcare & Percentage of outpatient visits with COVID diagnostic
codes or codes indicating COVID-like symptoms. Based on de-identified claims
data processed by Change Healthcare. & 2020-02-01 & County$^*$ \\
  Doctor Visits & Percentage of outpatient visits primarily about COVID-like
symptoms, based on de-identified claims data provided by health system
partners. & 2020-02-01 & County$^*$ \\
  Hospital Admissions & Percentage of new hospital admissions with COVID
diagnostic codes, based on de-identified claims data provided by health system
partners. & 2020-02-01 & County$^{**}$\\
  Quidel & Test positivity rates for COVID-19 antigen tests produced by
Quidel. & 2020-05-26 & County$^{**}$ \\
  SafeGraph & Mobility metrics, such as time away from home or visits to bars
and restaurants, based on cell phone mobility data collected by SafeGraph
\cite{SafeGraphSocial, SafeGraphPatterns}. & 2019-01-01 & County \\
  COVID Trends and Impact Survey & COVID symptoms, social distancing
behaviors, mental health, economic impact, behavior (e.g., mask-wearing,
vaccination attitudes), and COVID testing signals based on daily surveys
conducted nationally by Delphi through Facebook \cite{DelphiSurvey,
Kreuter:2020}. & 2020-04-06 & County$^{**}$ \\
  \midrule
  % Mirrored data from other sources
  Health \& Human Services & Counts of hospital admissions due to confirmed or
suspected COVID-19, as reported by the Department of Health \& Human Services. &
2019-12-31 & State \\
  COVID Act Now & COVID-19 testing results, such as positivity rate and number
of tests, compiled by COVID Act Now from CDC reporting. & 2020-03-02 &
County$^*$ \\
  Google Symptoms & Trends in Google search volume for terms related to
anosmia and ageusia (loss of smell or taste), which correlate with COVID
activity, based on data shared by Google \cite{GoogleSymptoms}. & 2020-02-13 &
County$^{***}$ \\
  Cases and Deaths & Confirmed COVID-19 cases and deaths, compiled by JHU CSSE
\cite{Dong:2020} and by USAFacts \cite{USAFacts}. & 2020-01-22 & County \\
    NCHS Mortality & Weekly totals of deaths broken down by cause, such as
COVID, flu, or pneumonia, compiled by the National Center for Health Statistics
\cite{NCHS}. & 2020-01-26 & State \\
  \bottomrule
\end{tabular}
\addtabletext{$^*$Available at $>60$\% of counties; $^{**}$available at
  20--60\% of counties; $^{***}$available at $<20$\% of counties.  For some
  signals, location availability varies over time, e.g., due to variable
  reporting volume.}
\label{tab:sources_signals}
\end{table*}

\subsection{Signal Processing}

Because each data source reports data in different formats, we must convert each
source to a common format. In this format, each record represents an observation
of one quantity at one time point in one location. Locations are coded
consistently using standard identifiers such as FIPS codes; the sample size and
standard error for each observation is also reported when applicable. Each
signal is reported at the finest geographic resolution its source supports (such
as county or state) and also aggregated to metropolitan statistical areas,
Health and Human Services regions, and hospital referral regions. National
averages are also provided. Crucially, each record is tagged with an
\textit{issue date} referring to when the value was first issued, as described
below. This allows tracking of revisions made to individual observations, as
each revision is tagged with its own issue date.

When appropriate, additional post-processing (often nontrivial) is applied to
the data. For example, data on visits to doctors' offices is subject to strong
day-of-week effects, and so regression is used to adjust for these
effects. Other indicators are available in raw versions and versions smoothed
with a 7-day trailing average. All processing is done using open-source code
written primarily in Python and R, and available publicly at
\url{https://github.com/cmu-delphi/covidcast-indicators/}.

\subsection{Revision Tracking}
\label{subsec:revision_tracking}

Many data sources that are useful for epidemic tracking are subject to revision
after their initial publication. For example, aggregated medical claims data may
be initially published after several days, but additional claims and corrections
may take days to weeks to be discovered, processed, and aggregated. Medical
testing data are also often subject to backlogs and reporting delays, and
estimates for any particular date are revised over time as errors are found or
additional data becomes available. This revision process is generally referred
to as \textit{backfill}.

For this reason, the COVIDcast API annotates every observation with two dates:
the \textit{time value}, the date the underlying events (such as tests or
doctor's visits) occurred, and the \textit{issue date} when Delphi aggregated
and reported the data for that time value. Importantly, there can be multiple
observations for a single time value with different issue dates, for example if
data is revised or claims records arrive late. Delphi tracks revisions to all
data sources we ingest, including external data sources (such as sources
tracking cases and deaths). Many external sources do not keep a public or
conveniently accessible record of revisions of their data.

For many purposes it is sufficient to use the most recently issued observation
at a given time value, and the COVIDcast API returns the most recent issue as
its default. However, for some applications it is crucial to know what was known
\textit{as of} a specific date. For example, an epidemic forecasting model will
be called upon to make its forecasts based on preliminary data about recent
trends, so when it is trained using historical data, it should be trained using
the initial versions of that data, not updates that would have been received
later. Moreover, these revision records allow models to be modified to account
for noise and bias in early data versions, or to exclude data that is too new to
be considered stable, and to ``rewind'' time and simulate how these revised
models would have performed using only the versions of data available \textit{as
  of} those times.

Research on data revisions in the context of influenza-like illness has shown
that backfill can significantly alter forecast performance \cite{Brooks:2018,
  Reich:2019}, and that careful training on preliminary data can reduce this
influence \cite{Brooks:2020}. Recent research has shown similar results for
COVID-19 forecasts \cite{Kamarthi:2021}. We also examine this in our companion
paper on forecasting, where we observe that training and validating models on
finalized data yields overly optimistic estimates of true test-time
performance.

\subsection{Public API}

The data described above is publicly available through the Delphi COVIDcast API
\cite{CovidcastAPI}.  By making HTTP requests specifying the data source,
signal, geographic level, and time period desired, users can receive data in
JSON or CSV form. For added convenience, we have written \texttt{covidcast} R
\cite{CovidcastR} and Python \cite{CovidcastPy} packages with functions to
request data, format it as a data frame, plot and map it, and combine it with
data from other sources.

The R and Python package software is public and open-source, at
\url{https://github.com/cmu-delphi/covidcast/}.  The API server software is
itself also public and open-source, at
\url{https://github.com/cmu-delphi/delphi-epidata/}.  Lastly, most data sources
are provided under the Creative Commons Attribution license, and a small number
have additional restrictions imposed by the data source; see
\url{https://cmu-delphi.github.io/delphi-epidata/api/covidcast_licensing.html}.

\subsection{Interactive Visualization}

Since April 2020, Delphi has been maintaining and continually improving various
online visualization tools for the COVIDcast indicators \cite{CovidcastViz}.
These tools fetch data directly from the API, and allow for exploration of both
temporal (e.g., time series graphs) and spatial (e.g., choropleth maps) trends
in the signals, as well as many other aspects, such as correlations, anomalies,
and backfill. There is also a dedicated dashboard for exploring results from the
COVID Trends and Impact Survey.

\subsection{COVID Forecasting}

Since July 2020, Delphi has been regularly submitting short-term forecasts of
COVID-19 case and death incidence, at the state and county levels, to the
COVID-19 Forecast Hub \cite{ForecastHub}, with ``CMU-TimeSeries'' as the
team-model name.  The process of building, training, and deploying our
forecasting models leverages much of the infrastructure described in this paper
(such as the COVIDcast API's \textit{as of} feature), and some of our
forecasting systems rely on auxiliary indicators (such as survey-based and
claims-based COVID-like illness signals, which are described below).

\section{Results}

The indicators that are available in the COVIDcast API have been used in
dashboards produced by COVID Act Now \cite{CovidActNow}, COVID Exit Strategy
\cite{CovidExitStrategy}, and others; to inform the Delphi, DeepCOVID
\cite{Rodriguez:2021}, and the Institute for Health Metrics and Evaluation
(IHME) \cite{IHMEProj} COVID forecasting models; in various federal and state
government reports and analyses; and in a range of news stories. Aside from
operational use in decision-making and forecasting, they have also facilitated
numerous analyses studying the impacts of COVID-19 on the public, the
effectiveness of policy interventions, and factors that influenced the spread of
the pandemic \cite{Adjodah:2021, Pierri:2021, Jewell:2021, Chakrabarti:2020,
  Doerr:2021}. The API currently serves hundreds of thousands of requests to
thousands of users every day.

In what follows, we present examples of the usefulness of some of the novel
signals available in the API. These examples demonstrate that such indicators
are meaningfully related to COVID activity, that they provide alternate views on
pandemic activity that are not subject to the same reporting glitches and delays
as traditional public health surveillance streams, and that they provide
information about public behavior and attitudes that are not available from any
other source. Code to reproduce all examples (which uses the \texttt{covidcast}
R package and fetches data from the API) can be found at
\url{https://github.com/cmu-delphi/covidcast-pnas/tree/main/indicators/code/}.

\subsection{Tracking Trends}

Many of the indicators in the COVIDcast API are intended to track COVID
activity. Five indicators in particular have the closest connections to
confirmed cases:

\begin{itemize}
\item Change Healthcare COVID-like illness (CHNG-CLI): The percentage of
  outpatient visits that are primarily about COVID-related symptoms, based on
  de-identified Change Healthcare claims data.
\item Change Healthcare COVID (CHNG-COVID): The percentage of outpatient visits
  with confirmed COVID-19, based on the same claims data.
\item COVID Trends and Impact Survey CLI (CTIS-CLI): The estimated percentage
  of the population with COVID-like illness based on Delphi's surveys of
  Facebook users.
\item COVID Trends and Impact Survey CLI in the community
  (CTIS-CLI-in-community): The estimated percentage of the population who know
  someone in their local community who is sick, based on the same surveys.
\item Quidel test positivity rate (Quidel-TPR): The percentage of positive
  results among Quidel COVID antigen tests.
\end{itemize}

Figure~\ref{fig:time_trends_national} compares the first three of these signals
to COVID cases in the United States (from JHU CSSE, smoothed with a 7-day
trailing average) over a year of the pandemic (April 15, 2020 to April 15,
2021), illustrating how they track national trends quite well.  Importantly,
this same relationship persists across multiple resolutions of the data, down to
smaller geographic regions such as states and counties, as shown in the
supplement.  This will also be illustrated in a more detailed correlation
analysis in the next subsection.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/time_trends_national.pdf}
  \caption{National trends, from April 2020 to April 2021, of four signals in
    the COVIDcast API. The auxiliary signals, based on medical claims data and
    massive surveys, track changes in officially reported cases quite
    well. (They have all been placed on the same scale as reported cases per
    100,000 people.)}
  \label{fig:time_trends_national}
\end{figure}

Besides tracking contemporaneous COVID activity, these and other indicators can
be used to improve forecasts of future COVID case trends, as investigated in a
companion paper.

\subsection{Correlation Analyses}

To quantify the ability of the signals described above to track trends in COVID
cases, we use the Spearman (rank) correlation and analyze two key correlation
patterns, between each signal and confirmed COVID case rates (cases per 100,000
people):

\begin{enumerate}
\item \textit{Geo-wise correlations} (i.e., on a specific date, do values of the
  signal correlate with case rates across locations?): Formally, let $X_t$ and
  $Y_t$ be vectors of values of a signal and case rates, over all locations, on
  date $t$. The geo-wise correlation at time $t$ is defined as $\cor(X_t,
  Y_t)$ (where here and throughout $\cor(\cdot,\cdot)$ denotes Spearman
  correlation). This examines whether a signal has the capability to help spot
  locations with high case rates at any given time.

\item \textit{Time-wise correlations} (i.e., at a specific location, do values
  of the signal correlate with case rates across time?): Let $X_\ell$
  and $Y_\ell$ be vectors of values of a signal and case rates, over all
  times, at location $\ell$. The time-wise correlation at location $\ell$ is
  defined as $\cor(X_\ell, Y_\ell)$. This examines whether changes in a signal
  over time correspond to changes in reported cases at the same
  location.
\end{enumerate}

Figure~\ref{fig:geo_wise_correlation} shows the geo-wise correlations achieved
by the five signals and COVID case rates (from JHU CSSE, smoothed using a 7-day
trailing average), from April 15, 2020 to April 15, 2021. This calculation is
performed over all counties with at least 500 cumulative cases by the end of
this period, and at which all indicators are available (956 counties in
total). The large positive correlations suggest that these signals could be
useful in hotspot detection (identifying counties that have relatively high
COVID activity, at a given time). Somewhat surprisingly, the survey-based
CLI-in-community signal shows the strongest correlations for much of the time
period. This clearly demonstrates the value of a large-scale survey such as CTIS
for tracking symptoms and case trends, especially when other data is unavailable.

Figure~\ref{fig:time_wise_correlation} summarizes time-wise correlations from
these five signals over the same time period, and for the same set of counties.
For each signal, we display the set of correlations that it achieves in
histogram form (more precisely, using a kernel density estimate). All signals
produce positive correlations in the majority of counties considered (with very
little mass in each estimated density being to the left of zero). The largest
correlations, in bulk, are achieved by the CHNG-COVID signal; the
CTIS-CLI-in-community signal is a close second, and the CHNG-CLI signal is
third. There are two noteworthy points:

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/geo_wise_corr.pdf}
  \caption{Geo-wise correlations with case rates, from April 15, 2020 to April
    15, 2021, calculated over all counties for which all signals were available
    and which had at least 500 cumulative cases by the end of this period.}
  \label{fig:geo_wise_correlation}
\end{figure}

\begin{itemize}
\item This is different from what is observed in
  Figure~\ref{fig:geo_wise_correlation}, where the CTIS-CLI-in-community signal
  achieves clearly the highest correlations for most of the time period.
  However, it is worth emphasizing that time-wise and geo-wise
  correlations are truly measuring different properties of a signal; and the
  claims signals (CHNG-COVID and CHNG-CLI) seem more appropriate for
  temporal---rather than spatial---comparisons.  We revisit this point in the
  discussion.

\item It is still quite impressive (and surprising) that the
  CTIS-CLI-in-community signal, based on people reporting on the symptoms of
  others around them, can achieve  nearly as strong time-wise correlations to
  confirmed cases as can a signal that is based on picking up the occurrence of
  a confirmed case passing through the outpatient system.
\end{itemize}

\subsection{Helping Robustness}

Public health reporting of COVID tests, cases, deaths, and hospitalizations is
subject to a number of possible delays and problems. For example, COVID testing
data is reported inconsistently by different states using different definitions
and inclusion criteria, and differences in reporting processes mean state data
often does not match data reported to the federal government
\cite{Schechtman:2021}. Case and death data is frequently backlogged and
corrected, resulting in artificial spikes and drops \cite{Simon:2021,
  ArvisaisAnhalt:2021}.

As an example, looking back at Figure \ref{fig:time_trends_national}, we can see
clear dips in the confirmed COVID case curve that occur around the Thanksgiving
and New Year's holidays. This is artificial, and due to the fact that public
health departments usually close over holiday periods, which delays case and
death reporting (for this reason, the artificial dips persist at the state- and
county-level as well). The CLI signal from the survey, on the other hand,
displays no such dips. The claims signals actually display holiday effects going
in the other direction: they exhibit \textit{spikes} around Thanksgiving and New
Year's. This is because they measure the fraction of all outpatient visits with
a certain condition, and the denominator here (total outpatient visits) drops
disproportionately during holiday periods, as people are likely less willing to
go to the doctor for more routine issues. Fortunately, in principle, the holiday
effects in claims signals should be correctable: they are mainly due to
\textit{overall} changes in medical seeking behavior during holiday, periods,
and we can estimate such effects using historical claims data.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/time_wise_correlation.pdf}
  \caption{Time-wise correlations with case rates, from April 15, 2020 to April
    15, 2021, calculated over all counties for which all signals were available
    and which had at least 500 cumulative cases by the end of this period.}
  \label{fig:time_wise_correlation}
\end{figure}

As a further example, Figure~\ref{fig:bexar_compare} displays data from Bexar
County, Texas (which contains San Antonio) during July 2020. On July 16, 2020,
San Antonio reported 4,810 backlogged cases after reporting problems prevented
them from being reported over the past two weeks \cite{Palacios:2021}, resulting
in a clearly visible spike in the left-hand panel of the figure (case data from
JHU CSSE, smoothed using a 7-day trailing average). Meanwhile, Delphi's COVID
Trends and Impact Survey averaged around 350 responses per day in Bexar County
over the same time period, and was able to estimate the fraction of the
population who know someone in their local community with COVID-Like Illness
(CLI). As we can see in the right-hand panel of the figure, this signal was not
affected by Bexar County's reporting problems and, as shown in the last
subsection, it is (in general) highly correlated with case rates, providing an
alternate stream of data about COVID activity unaffected by backlogs. Similar
reporting problems have occurred in many jurisdictions across the United States,
making it valuable to cross-check against external sources not part of the same
reporting systems.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/bexar_compare.pdf}
  \caption{Reported cases per day in Bexar County, Texas during the summer of
    2020. On July 16, 4,810 backlogged cases were reported, though they actually
    occurred over the preceding two weeks (this shows up as a prolonged spike in
    the left panel due to the 7-day trailing averaging applied to the case
    counts). Daily CTIS estimates of CLI-in-community showed more stable
    underlying trends.}
  \label{fig:bexar_compare}
\end{figure}

\subsection{Revisions Matter}

The revision tracking feature in the API assists in model-building and
evaluation. Figure~\ref{fig:dv_as_of} illustrates how one COVIDcast medical
claims signal evolved as it was revised across multiple issue dates, in four
different states, between June 1 and August 1, 2020. In each plot, the rightmost
ends of the lines correspond to estimates for the last day that data are
available for each issue date, which are generally the most tentative estimates,
and appear to be significantly biased upward in Arizona in June 2020, and
significantly biased downward in New York throughout June and July 2020.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{fig/dv_as_of.pdf}
  \caption{Estimated percentage of doctor's visits due to COVID-like illness
    displayed across multiple issue dates, with later issue dates adding
    additional data and revising past data from prior issue dates.}
  \label{fig:dv_as_of}
\end{figure}

Claims-based signals typically undergo heavy backfill as additional claims are
processed and errors are corrected; the median relative error between initial
reports and final values is over 20\% for such data, and only after roughly 35
days do estimates typically match finalized values within 5\%. However, the
systematic nature of this backfill, as illustrated in Figure~\ref{fig:dv_as_of},
suggests that statistical models could be fit (potentially separately for each
location) to estimate the final values from preliminary reports. On the other
hand, official public health reporting of COVID cases and deaths can be subject
to revision as death certificates are audited and backlogs cleared, resulting in
thousands of cases and deaths being added or removed. This process is much more
difficult to predict, and thus claims data and other sources may be a useful
stand-in while public health reports are aggregated and corrected.

To reiterate a previous point, when training and validating forecast models (on
historical data), users will want to use data that was known \textit{as of} the
forecast date, not revised versions that only became available much later. The
COVIDcast API makes all historical versions available and easily accessible for
this purpose; and this feature plays a prominent role in our own analysis of
forecasting and hotspot prediction models appearing in a companion paper.

\subsection{New Perspectives}

Auxiliary signals (outside of the standard public health reporting streams) can
serve as indicators of COVID activity, but they can also illustrate the effect
of mitigating actions (such as shelter-in-place orders) and can guide resource
allocation for fighting the pandemic. For example, medical claims data reflects
healthcare-seeking behavior; measures of mobility reflect adherence to public
health recommendations; and measures of COVID vaccine acceptance can guide
outreach efforts.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{fig/mobility-drop.pdf}
  \caption{Percentage point increase in SafeGraph estimates of the percentage of
  people staying completely at home, based on aggregated mobile device data,
  between March 1, 2020 and April 15, 2020.}
  \label{fig:mobility-drop}
\end{figure}

As an illustration, Figure~\ref{fig:mobility-drop} maps the sharp increase in
rates of people staying at home between March 1 and April 15, 2020. Using
SafeGraph data on the fraction of mobile devices included in SafeGraph's panel
that did not leave the immediate area of their home, it illustrates the sharp
drop in travel and work outside the home that occurred in the early stages of
the pandemic. It also shows that this drop was much more pronounced in some
states than others, enabling analysis of policy impacts and disease spread.
Similar maps can be quickly constructed for any signal in the COVIDcast API.

\section{Discussion}

The COVIDcast API provides open access to real-time and geographically-detailed
indicators of COVID activity in the United States, which supports and enhances
standard public health reporting streams in several ways.

First, several signals in the API closely track COVID activity (over both time
and space); yet they are derived from different data streams (such as surveys,
medical insurance claims, and medical devices), and are thus not subject to the
same sources of error as public health reporting streams. This can be important
both for robustness and situational awareness, allowing decision-makers to
diagnose potential anomalies in standard surveillance streams, and for modeling
tasks such as forecasting and nowcasting. Our companion paper on forecasting
discusses this in more detail.

Second, the API features many other signals that are relevant to understanding
aspects of the pandemic and its effects on the United States population that are
not found in traditional public health streams, such as data on mobility
patterns, internet search trends, mask wearing, and vaccine hesitancy, to name
just a few. (The latter two signals are derived from the COVID Trends and Impact
Survey; our companion paper on this survey gives a more detailed view of its
features and capabilities.) These signals have already supported pandemic
research and  policy-making.

Third, the underlying database tracks all revisions made to the data, allowing
us to query the API to learn ``what was known when,'' which is critical for
understanding the behavior (and potential pitfalls) of real-time surveillance
signals. Such revision data is rarely available in standardized format from
other sources.

Finally, we emphasize that unifying many relevant signals into a single common
format, with comprehensive revision tracking, is an important goal in and of
itself. The ability to combine public health reporting data, syndromic
surveillance data, and digital measures of mobility and behaviors goes beyond
providing traditional situational awareness.  Convenient and real-time access to
this data enables continuous telemetry summarizing how things are, how they are
expected to change, which areas need additional resources to be allocated in
response, and how effective public communication is.

There are a number of open questions, and challenges that remain. Several
signals are subject to biases, such as survey sampling and nonresponse biases,
geographic differences in market share for medical claims data, or biases in the
population represented in app-based mobility data.
% \cite{Coston:2021}. RJT: commenting out because if we cite only this, then I
% feel like we will have to provided references for the other sources of bias.
% Since otherwise it might look like we don't think there's credible work on the
% others.
Claims data tends also to be subject to biases during major national holidays
and other events that change health-seeking behavior. Characterizing these
biases will be important for future research and operational systems that use
these signals.  Several data sources are also subject to extensive revision and
backfill, which must be studied and modeled to enable effective real-time use of
these sources in forecasting and nowcasting systems. The breadth and unique
features of the COVIDcast API will help facilitate this and other related work,
which will be vital to advancing pandemic modeling and preparedness.

\acknow{We thank Carrie Reed, Matt Biggerstaff, Michael Johansson, Rachel
  Slayton, Velma Lopez, Jo Walker and others on the CDC COVID-19 Modeling Team;
  Hal Varian, Brett Slatkin, and others on Google's Surveys team;
  Erin Hattersley, Rasmi Elasmar, and others at Google.org;
  Evgeniy Gabrilovich and others on Google's Health team;
  Curtiss Cobb and others on Facebook's  Demography and Survey Science, Data for
  Good and Health teams;
  Alex Smola and others at Amazon Web Services;
  Tim Suther and others at Change Healthcare;
  Paul Nielsen and others at Optum;
  John Tamerius and others at Quidel; and
  Ross Epstein and others at SafeGraph.
  This material is based on work supported by gifts from Facebook, Google.org,
  the McCune Foundation, and Optum; Centers for Disease Control and Prevention
  (CDC) grant U01IP001121; National Science Foundation Graduate Research
  Fellowship Program (NSF GRFP) award DGE1745016; and the Center for Machine
  Learning and Health (CMLH) at Carnegie Mellon.}

\showacknow{} % Display the acknowledgments section

% Bibliography
\bibliography{../../common/covidcast.bib}

\end{document}
